<!DOCTYPE html><html lang="zh-CN" data-default-color-scheme="auto"><head><meta charset="UTF-8"><link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png"><link rel="icon" href="/source/img/fluid.png"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=5,shrink-to-fit=no"><meta http-equiv="x-ua-compatible" content="ie=edge"><meta name="theme-color" content="#2f4154"><meta name="author" content="Jimes"><meta name="keywords" content=""><meta name="description" content="一步一步带你实现BP神经网络（数学）"><meta property="og:type" content="article"><meta property="og:title" content="神经网络基础（带数学公式、代码）"><meta property="og:url" content="https://jimes.cn/2025/01/19/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80%EF%BC%88%E5%B8%A6%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F%EF%BC%89/index.html"><meta property="og:site_name" content="马锦的博客"><meta property="og:description" content="一步一步带你实现BP神经网络（数学）"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://jimes.cn/2025/01/19/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80%EF%BC%88%E5%B8%A6%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F%EF%BC%89/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84.jpg"><meta property="og:image" content="https://jimes.cn/2025/01/19/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80%EF%BC%88%E5%B8%A6%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F%EF%BC%89/%E7%A5%9E%E7%BB%8F%E5%85%83.jpg"><meta property="og:image" content="https://jimes.cn/2025/01/19/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80%EF%BC%88%E5%B8%A6%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F%EF%BC%89/%E7%A5%9E%E7%BB%8F%E5%85%83%E7%BB%84%E5%90%88.jpg"><meta property="og:image" content="https://jimes.cn/2025/01/19/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80%EF%BC%88%E5%B8%A6%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F%EF%BC%89/sigmoid-1.jpg"><meta property="og:image" content="https://jimes.cn/2025/01/19/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80%EF%BC%88%E5%B8%A6%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F%EF%BC%89/tanh-1.jpg"><meta property="og:image" content="https://jimes.cn/2025/01/19/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80%EF%BC%88%E5%B8%A6%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F%EF%BC%89/relu-1.jpg"><meta property="og:image" content="https://jimes.cn/2025/01/19/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80%EF%BC%88%E5%B8%A6%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F%EF%BC%89/LRelu-1.jpg"><meta property="og:image" content="https://jimes.cn/2025/01/19/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80%EF%BC%88%E5%B8%A6%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F%EF%BC%89/softmax-1.jpg"><meta property="article:published_time" content="2025-01-19T09:56:15.374Z"><meta property="article:modified_time" content="2025-02-16T13:36:59.767Z"><meta property="article:author" content="Jimes"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:image" content="https://jimes.cn/2025/01/19/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80%EF%BC%88%E5%B8%A6%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F%EF%BC%89/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84.jpg"><meta name="referrer" content="no-referrer-when-downgrade"><title>神经网络基础（带数学公式、代码） - 马锦的博客</title><link rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css"><link rel="stylesheet" href="/css/main.css"><link id="highlight-css" rel="stylesheet" href="/css/highlight.css"><link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css"><script id="fluid-configs">var dntVal,Fluid=window.Fluid||{},CONFIG=(Fluid.ctx=Object.assign({},Fluid.ctx),{hostname:"jimes.cn",root:"/",version:"1.9.7",typing:{enable:!0,typeSpeed:70,cursorChar:"_",loop:!1,scope:[]},anchorjs:{enable:!0,element:"h1,h2,h3,h4,h5,h6",placement:"left",visible:"hover",icon:""},progressbar:{enable:!0,height_px:3,color:"#29d",options:{showSpinner:!1,trickleSpeed:100}},code_language:{enable:!0,default:"TEXT"},copy_btn:!0,image_caption:{enable:!0},image_zoom:{enable:!0,img_url_replace:["",""]},toc:{enable:!0,placement:"right",headingSelector:"h1,h2,h3,h4,h5,h6",collapseDepth:0},lazyload:{enable:!0,loading_img:"/img/loading.gif",onlypost:!1,offset_factor:2},web_analytics:{enable:!0,follow_dnt:!1,baidu:null,google:{measurement_id:null},tencent:{sid:null,cid:null},woyaola:null,cnzz:null,leancloud:{app_id:null,app_key:null,server_url:null,path:"window.location.pathname",ignore_local:!0}},search_path:"https://cdn.jsdelivr.net/gh/jimes3/jimes3.github.io/local-search.xml",include_content_in_search:!0});CONFIG.web_analytics.follow_dnt&&(dntVal=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,Fluid.ctx.dnt=dntVal&&(dntVal.startsWith("1")||dntVal.startsWith("yes")||dntVal.startsWith("on")))</script><script src="/js/utils.js"></script><script src="/js/color-schema.js"></script><script async>Fluid.ctx.dnt||Fluid.utils.createScript("https://www.googletagmanager.com/gtag/js?id=",function(){function a(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],a("js",new Date),a("config","")})</script><meta name="generator" content="Hexo 7.0.0"></head><body><header><div class="header-inner" style="height:70vh"><nav id="navbar" class="navbar fixed-top navbar-expand-lg navbar-dark scrolling-navbar"><div class="container"><a class="navbar-brand" href="/"><strong class="navbar-title">Jimes&#39; Blog</strong> </a><button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><div class="animated-icon"><span></span><span></span><span></span></div></button><div class="collapse navbar-collapse" id="navbarSupportedContent"><ul class="navbar-nav ml-auto text-center"><li class="nav-item"><a class="nav-link" href="/" target="_self"><i class="iconfont icon-home-fill"></i> <span>首页</span></a></li><li class="nav-item"><a class="nav-link" href="/archives/" target="_self"><i class="iconfont icon-archive-fill"></i> <span>归档</span></a></li><li class="nav-item"><a class="nav-link" href="/categories/" target="_self"><i class="iconfont icon-category-fill"></i> <span>分类</span></a></li><li class="nav-item"><a class="nav-link" href="/tags/" target="_self"><i class="iconfont icon-tags-fill"></i> <span>标签</span></a></li><li class="nav-item"><a class="nav-link" href="/about/" target="_self"><i class="iconfont icon-user-fill"></i> <span>关于</span></a></li><li class="nav-item" id="search-btn"><a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search"><i class="iconfont icon-search"></i></a></li><li class="nav-item" id="color-toggle-btn"><a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle"><i class="iconfont icon-dark" id="color-toggle-icon"></i></a></li></ul></div></div></nav><div id="banner" class="banner" parallax="true" style="background:url(/img/default.png) no-repeat center center;background-size:cover"><div class="full-bg-img"><div class="mask flex-center" style="background-color:rgba(0,0,0,.3)"><div class="banner-text text-center fade-in-up"><div class="h2"><span id="subtitle" data-typed-text="神经网络基础（带数学公式、代码）"></span></div><div class="mt-3"><span class="post-meta"><i class="iconfont icon-date-fill" aria-hidden="true"></i> <time datetime="2025-01-19 17:56" pubdate>2025年1月19日 下午</time></span></div><div class="mt-1"><span class="post-meta mr-2"><i class="iconfont icon-chart"></i> 1.9k 字 </span><span class="post-meta mr-2"><i class="iconfont icon-clock-fill"></i> 32 分钟 </span><span id="busuanzi_container_page_pv" style="display:none"><i class="iconfont icon-eye" aria-hidden="true"></i> <span id="busuanzi_value_page_pv"></span> 次</span></div></div></div></div></div></div></header><main><div class="container-fluid nopadding-x"><div class="row nomargin-x"><div class="side-col d-none d-lg-block col-lg-2"></div><div class="col-lg-8 nopadding-x-md"><div class="container nopadding-x-md" id="board-ctn"><div id="board"><article class="post-content mx-auto"><h1 id="seo-header">神经网络基础（带数学公式、代码）</h1><p id="updated-time" class="note note-info">本文最后更新于 2025年2月16日 晚上</p><div class="markdown-body"><h1 id="一，神经网络结构"><a href="#一，神经网络结构" class="headerlink" title="一，神经网络结构"></a>一，神经网络结构</h1><img src="/images/loading.png" srcset="/img/loading.gif" lazyload data-original="/2025/01/19/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80%EF%BC%88%E5%B8%A6%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F%EF%BC%89/神经网络结构.jpg" width="40%"><ul><li>输入层：最简单的一层，每个结点就是一个变量，结点值就是变量值。</li><li>隐藏层：最复杂的一层，可以有多层，根据实际问题复杂情况进行选择。每个结点都为一个神经元。</li><li>输出层：如果是分类任务，输出层的结点数即为类别数，每个输出层结点也是一个神经元，但还附带分类器实现值到概率的转换。</li></ul><h1 id="二，前向传播（神经元的计算过程）"><a href="#二，前向传播（神经元的计算过程）" class="headerlink" title="二，前向传播（神经元的计算过程）"></a>二，前向传播（神经元的计算过程）</h1><p><img src="/images/loading.png" srcset="/img/loading.gif" lazyload data-original="/2025/01/19/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80%EF%BC%88%E5%B8%A6%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F%EF%BC%89/神经元.jpg" width="40%"> <img src="/images/loading.png" srcset="/img/loading.gif" lazyload data-original="/2025/01/19/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80%EF%BC%88%E5%B8%A6%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F%EF%BC%89/神经元组合.jpg" width="40%"></p><ul><li>前一层每个结点都有一个权值，Sum即为前一层所有结点的带权求和，Sgn即为激活函数，其目的是实现非线性组合以及实现最小梯度下降算法满足反向传播条件。</li></ul><h2 id="2-1激活函数"><a href="#2-1激活函数" class="headerlink" title="2.1激活函数"></a>2.1激活函数</h2><h3 id="2-1-1-饱和神经元激活函数"><a href="#2-1-1-饱和神经元激活函数" class="headerlink" title="2.1.1 饱和神经元激活函数"></a>2.1.1 饱和神经元激活函数</h3><ul><li>饱和的含义是任意 x 的输入，其对应的 y 输出的取值范围都位于一个区间内。</li></ul><ol><li>Sigmoid函数 $$ f(x)&#x3D;\frac{1}{1+e^{-x} } $$ <img src="/images/loading.png" srcset="/img/loading.gif" lazyload data-original="/2025/01/19/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80%EF%BC%88%E5%B8%A6%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F%EF%BC%89/sigmoid-1.jpg"><ul><li>函数输出范围是[0，1]，已经不太受欢迎了</li><li>函数输出不是以 0 为中心的，梯度可能就会向特定方向移动，从而降低权重更新的效率。</li><li>函数执行指数运算，计算机运行得较慢，比较消耗计算资源。</li></ul></li><li>Tanh函数 $$ f(x)&#x3D;\frac{e^{x}-e^{-x} }{e^{x}+e^{-x}} $$ <img src="/images/loading.png" srcset="/img/loading.gif" lazyload data-original="/2025/01/19/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80%EF%BC%88%E5%B8%A6%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F%EF%BC%89/tanh-1.jpg"><ul><li>函数输出范围是[-1，1]，以 0 为中心，不会向特定方向移动</li><li>依然进行的是指数运算</li></ul></li></ol><ul><li>上述两种激活函数在 x 值离中心较远时均会出现梯度消失现象。</li></ul><h3 id="2-1-2-非饱和神经元激活函数"><a href="#2-1-2-非饱和神经元激活函数" class="headerlink" title="2.1.2 非饱和神经元激活函数"></a>2.1.2 非饱和神经元激活函数</h3><ol><li>Relu函数 $$ f(x)&#x3D;\max (0,x) $$ <img src="/images/loading.png" srcset="/img/loading.gif" lazyload data-original="/2025/01/19/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80%EF%BC%88%E5%B8%A6%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F%EF%BC%89/relu-1.jpg"><ul><li>输出不是以0为中心的.</li><li>当输入为负时，梯度为0。这个神经元及之后的神经元梯度永远为0，不再对任何数据有所响应。因此学习率应当设置得较小。</li></ul></li><li>Leaky Relu函数 $$ f(x)&#x3D;\max (\alpha x,x) $$ <img src="/images/loading.png" srcset="/img/loading.gif" lazyload data-original="/2025/01/19/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80%EF%BC%88%E5%B8%A6%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F%EF%BC%89/LRelu-1.jpg"><ul><li>函数中的α，需要通过先验知识人工赋值（一般设为0.01）</li><li>有些近似线性，导致在复杂分类中效果不好</li><li>尚未完全证明 Leaky ReLU 总是比 ReLU 更好</li></ul></li></ol><h1 id="三，反向传播（链式法则）"><a href="#三，反向传播（链式法则）" class="headerlink" title="三，反向传播（链式法则）"></a>三，反向传播（链式法则）</h1><ul><li>损失函数对前一层对应结点的变量求偏导，中间隔着激活函数就用链式法则。</li><li>对于设定的步长，偏导函数值的<strong>相反数</strong>即为权重调整的大小。</li><li>用相反数是是为了满足变量的变化与函数变化的关系。</li></ul><h2 id="1-数学背景"><a href="#1-数学背景" class="headerlink" title="1. 数学背景"></a>1. 数学背景</h2><p>在神经网络中，每一层的线性输出 $ Z $ 和激活值 $ A $ 的关系如下：</p><p>$$<br>Z &#x3D; X \cdot W + b<br>$$</p><p>$$<br>A &#x3D; \sigma(Z)<br>$$</p><p>其中：</p><ul><li>$ X $ 是输入（或前一层的激活值）。</li><li>$ W $ 是权重矩阵。</li><li>$ b $ 是偏置向量。</li><li>$ \sigma $ 是激活函数（如 Sigmoid）。</li></ul><p>损失函数 $ L $ 对权重 $ W $ 的偏导数 $ \frac{\partial L}{\partial W} $ 可以通过链式法则计算：</p><p>$$<br>\frac{\partial L}{\partial W} &#x3D; \frac{\partial L}{\partial Z} \cdot \frac{\partial Z}{\partial W}<br>$$</p><hr><h2 id="2-链式法则的展开"><a href="#2-链式法则的展开" class="headerlink" title="2. 链式法则的展开"></a>2. 链式法则的展开</h2><h3 id="1-计算-frac-partial-L-partial-Z"><a href="#1-计算-frac-partial-L-partial-Z" class="headerlink" title="(1) 计算 $ \frac{\partial L}{\partial Z} $"></a>(1) 计算 $ \frac{\partial L}{\partial Z} $</h3><ul><li>$ \frac{\partial L}{\partial Z} $ 是损失函数对线性输出 $ Z $ 的偏导数，通常记作 $ dZ $。</li><li>对于输出层，$ dZ $ 可以直接通过损失函数和激活函数的导数计算得到。</li><li>对于隐藏层，$ dZ $ 通过后一层的梯度传播而来。</li></ul><h3 id="2-计算-frac-partial-Z-partial-W"><a href="#2-计算-frac-partial-Z-partial-W" class="headerlink" title="(2) 计算 $ \frac{\partial Z}{\partial W} $"></a>(2) 计算 $ \frac{\partial Z}{\partial W} $</h3><ul><li><p>线性输出 $ Z $ 对权重 $ W $ 的偏导数是输入 $ X $（或前一层的激活值 $ A_{\text{prev}} $）。</p></li><li><p>这是因为：</p><p>$$<br>Z &#x3D; X \cdot W + b<br>$$</p><p>对 $ W $ 求偏导时，$ X $ 是常数，因此：</p><p>$$<br>\frac{\partial Z}{\partial W} &#x3D; X^T<br>$$</p></li></ul><h2 id="3-总结"><a href="#3-总结" class="headerlink" title="3. 总结"></a>3. 总结</h2><h3 id="1-前向传播"><a href="#1-前向传播" class="headerlink" title="(1) 前向传播"></a>(1) 前向传播</h3><p>$$<br>Z &#x3D; X \cdot W + b<br>$$</p><p>$$<br>A &#x3D; \sigma(Z)<br>$$</p><h3 id="2-反向传播"><a href="#2-反向传播" class="headerlink" title="(2) 反向传播"></a>(2) 反向传播</h3><ol><li><p>计算输出层的梯度 $ dZ $：</p><p>$$<br>dZ &#x3D; \frac{\partial L}{\partial Z} &#x3D; A - Y<br>$$</p></li><li><p>计算权重梯度 $ dW $：</p><p>$$<br>dW &#x3D; \frac{\partial L}{\partial W} &#x3D; X^T \cdot dZ<br>$$</p></li><li><p>更新权重：</p><p>$$<br>W &#x3D; W - \alpha \cdot dW<br>$$</p></li></ol><h1 id="四，Softmax分类器"><a href="#四，Softmax分类器" class="headerlink" title="四，Softmax分类器"></a>四，Softmax分类器</h1><p>Softmax函数：$$ Softmax(x)&#x3D;\frac{e^{x_i} }{ {\textstyle \sum_{i}^{}e^{x_i}} } $$ <img src="/images/loading.png" srcset="/img/loading.gif" lazyload data-original="/2025/01/19/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80%EF%BC%88%E5%B8%A6%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F%EF%BC%89/softmax-1.jpg"></p><ul><li>Softmax函数一般用于最后一层输出层，实现将最后的计算值转化为不同类别的概率，其中概率最大的就是预测类别。</li><li>用指数函数进行累加是为了增大不同值之间的区分度。</li></ul><h1 id="五，代码"><a href="#五，代码" class="headerlink" title="五，代码"></a>五，代码</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">sigmoid</span>(<span class="hljs-params">x</span>):<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">1</span> / (<span class="hljs-number">1</span> + np.exp(-x))<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">sigmoid_derivative</span>(<span class="hljs-params">sigmoid_x</span>):<br>    <span class="hljs-keyword">return</span> sigmoid_x * (<span class="hljs-number">1</span> - sigmoid_x)<br><br><span class="hljs-comment"># 初始化网络参数</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">initialize_parameters</span>(<span class="hljs-params">layer_dims</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    layer_dims: 列表，表示每一层的神经元数量，例如 [input_size, hidden_size1, hidden_size2, ..., output_size]</span><br><span class="hljs-string">    return: 参数字典，包含每一层的 W 和 b</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    np.random.seed(<span class="hljs-number">42</span>)<br>    parameters = &#123;&#125;<br>    L = <span class="hljs-built_in">len</span>(layer_dims)  <span class="hljs-comment"># 网络的总层数</span><br><br>    <span class="hljs-keyword">for</span> l <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, L):<br>        parameters[<span class="hljs-string">f&#x27;W<span class="hljs-subst">&#123;l&#125;</span>&#x27;</span>] = np.random.randn(layer_dims[l-<span class="hljs-number">1</span>], layer_dims[l]) * <span class="hljs-number">0.01</span><br>        parameters[<span class="hljs-string">f&#x27;b<span class="hljs-subst">&#123;l&#125;</span>&#x27;</span>] = np.zeros((<span class="hljs-number">1</span>, layer_dims[l]))<br><br>    <span class="hljs-keyword">return</span> parameters<br><br><span class="hljs-comment"># 前向传播</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">forward_propagation</span>(<span class="hljs-params">X, parameters</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    X: 输入数据</span><br><span class="hljs-string">    parameters: 参数字典</span><br><span class="hljs-string">    return: 最后一层的输出 A，以及缓存（包含每一层的 Z 和 A）</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    caches = []<br>    A = X<br>    L = <span class="hljs-built_in">len</span>(parameters) // <span class="hljs-number">2</span>  <span class="hljs-comment"># 网络的总层数（不包括输入层）</span><br><br>    <span class="hljs-keyword">for</span> l <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, L+<span class="hljs-number">1</span>):<br>        W = parameters[<span class="hljs-string">f&#x27;W<span class="hljs-subst">&#123;l&#125;</span>&#x27;</span>]<br>        b = parameters[<span class="hljs-string">f&#x27;b<span class="hljs-subst">&#123;l&#125;</span>&#x27;</span>]<br>        Z = np.dot(A, W) + b  <span class="hljs-comment"># 全连接</span><br>        A = sigmoid(Z)  <span class="hljs-comment"># 激活函数</span><br>        caches.append((Z, A))  <span class="hljs-comment"># 缓存每一层的 Z 和 A</span><br><br>    <span class="hljs-keyword">return</span> A, caches<br><br><span class="hljs-comment"># 计算损失</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_loss</span>(<span class="hljs-params">A2, Y</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    A2: 最后一层的输出</span><br><span class="hljs-string">    Y: 真实标签</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    m = Y.shape[<span class="hljs-number">0</span>]<br>    loss = -np.<span class="hljs-built_in">sum</span>(Y * np.log(A2) + (<span class="hljs-number">1</span> - Y) * np.log(<span class="hljs-number">1</span> - A2)) / m<br>    <span class="hljs-keyword">return</span> loss<br><br><span class="hljs-comment"># 反向传播</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">backward_propagation</span>(<span class="hljs-params">X, Y, parameters, caches</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    X: 输入数据</span><br><span class="hljs-string">    Y: 真实标签</span><br><span class="hljs-string">    parameters: 参数字典</span><br><span class="hljs-string">    caches: 缓存（包含每一层的 Z 和 A）</span><br><span class="hljs-string">    return: 梯度字典，包含每一层的 dW 和 db</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    gradients = &#123;&#125;<br>    L = <span class="hljs-built_in">len</span>(parameters) // <span class="hljs-number">2</span>  <span class="hljs-comment"># 网络的总层数（不包括输入层）</span><br>    m = X.shape[<span class="hljs-number">0</span>]<br><br>    <span class="hljs-comment"># 最后一层的梯度</span><br>    A = caches[-<span class="hljs-number">1</span>][<span class="hljs-number">1</span>]  <span class="hljs-comment"># 最后一层的输出</span><br>    dZ = A - Y<br>    dW = np.dot(caches[-<span class="hljs-number">2</span>][<span class="hljs-number">1</span>].T, dZ) / m  <span class="hljs-comment"># 倒数第二层的 A 是倒数第一层的输入</span><br>    db = np.<span class="hljs-built_in">sum</span>(dZ, axis=<span class="hljs-number">0</span>, keepdims=<span class="hljs-literal">True</span>) / m<br>    gradients[<span class="hljs-string">f&#x27;dW<span class="hljs-subst">&#123;L&#125;</span>&#x27;</span>] = dW<br>    gradients[<span class="hljs-string">f&#x27;db<span class="hljs-subst">&#123;L&#125;</span>&#x27;</span>] = db<br><br>    <span class="hljs-comment"># 从倒数第二层到第一层的梯度</span><br>    <span class="hljs-keyword">for</span> l <span class="hljs-keyword">in</span> <span class="hljs-built_in">reversed</span>(<span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, L)):<br>        dA_prev = np.dot(dZ, parameters[<span class="hljs-string">f&#x27;W<span class="hljs-subst">&#123;l+<span class="hljs-number">1</span>&#125;</span>&#x27;</span>].T) <span class="hljs-comment"># 变化量的大小</span><br>        dZ = dA_prev * sigmoid_derivative(caches[l-<span class="hljs-number">1</span>][<span class="hljs-number">1</span>])  <span class="hljs-comment"># 变化量与偏导值的乘积，即反向传播的变化量</span><br>        dW = np.dot(caches[l-<span class="hljs-number">2</span>][<span class="hljs-number">1</span>].T, dZ) / m <span class="hljs-keyword">if</span> l &gt; <span class="hljs-number">1</span> <span class="hljs-keyword">else</span> np.dot(X.T, dZ) / m <span class="hljs-comment"># 损失函数对权重 W 的偏导数，Z=WX+B</span><br>        db = np.<span class="hljs-built_in">sum</span>(dZ, axis=<span class="hljs-number">0</span>, keepdims=<span class="hljs-literal">True</span>) / m<br>        gradients[<span class="hljs-string">f&#x27;dW<span class="hljs-subst">&#123;l&#125;</span>&#x27;</span>] = dW<br>        gradients[<span class="hljs-string">f&#x27;db<span class="hljs-subst">&#123;l&#125;</span>&#x27;</span>] = db<br><br>    <span class="hljs-keyword">return</span> gradients<br><br><span class="hljs-comment"># 更新参数</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">update_parameters</span>(<span class="hljs-params">parameters, gradients, learning_rate</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    parameters: 参数字典</span><br><span class="hljs-string">    gradients: 梯度字典</span><br><span class="hljs-string">    learning_rate: 学习率</span><br><span class="hljs-string">    return: 更新后的参数字典</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    L = <span class="hljs-built_in">len</span>(parameters) // <span class="hljs-number">2</span><br><br>    <span class="hljs-keyword">for</span> l <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, L+<span class="hljs-number">1</span>):<br>        parameters[<span class="hljs-string">f&#x27;W<span class="hljs-subst">&#123;l&#125;</span>&#x27;</span>] -= learning_rate * gradients[<span class="hljs-string">f&#x27;dW<span class="hljs-subst">&#123;l&#125;</span>&#x27;</span>] <span class="hljs-comment"># 梯度下降</span><br>        parameters[<span class="hljs-string">f&#x27;b<span class="hljs-subst">&#123;l&#125;</span>&#x27;</span>] -= learning_rate * gradients[<span class="hljs-string">f&#x27;db<span class="hljs-subst">&#123;l&#125;</span>&#x27;</span>] <span class="hljs-comment"># 梯度下降</span><br><br>    <span class="hljs-keyword">return</span> parameters<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">train</span>(<span class="hljs-params">X, Y, layer_dims, learning_rate, epochs</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    X: 输入数据</span><br><span class="hljs-string">    Y: 真实标签</span><br><span class="hljs-string">    layer_dims: 列表，表示每一层的神经元数量</span><br><span class="hljs-string">    learning_rate: 学习率</span><br><span class="hljs-string">    epochs: 训练轮数</span><br><span class="hljs-string">    return: 训练后的参数字典</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    parameters = initialize_parameters(layer_dims)<br><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(epochs):<br>        A, caches = forward_propagation(X, parameters)<br>        loss = compute_loss(A, Y)<br>        gradients = backward_propagation(X, Y, parameters, caches)<br>        parameters = update_parameters(parameters, gradients, learning_rate)<br>        <span class="hljs-built_in">print</span>(gradients)<br>        <span class="hljs-keyword">if</span> i % <span class="hljs-number">1000</span> == <span class="hljs-number">0</span>:<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Epoch <span class="hljs-subst">&#123;i&#125;</span>, Loss: <span class="hljs-subst">&#123;loss&#125;</span>&quot;</span>)<br><br>    <span class="hljs-keyword">return</span> parameters<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">predict</span>(<span class="hljs-params">X, parameters</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    X: 输入数据</span><br><span class="hljs-string">    parameters: 参数字典</span><br><span class="hljs-string">    return: 预测结果</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    A, _ = forward_propagation(X, parameters)<br>    predictions = np.<span class="hljs-built_in">round</span>(A)<br>    <span class="hljs-keyword">return</span> predictions<br><br><span class="hljs-comment"># 示例数据</span><br>X = np.array([[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>], [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>], [<span class="hljs-number">1</span>, <span class="hljs-number">0</span>], [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>]])<br>Y = np.array([[<span class="hljs-number">0</span>], [<span class="hljs-number">1</span>], [<span class="hljs-number">1</span>], [<span class="hljs-number">0</span>]])<br><br><span class="hljs-comment"># 定义网络结构</span><br>layer_dims = [<span class="hljs-number">2</span>, <span class="hljs-number">4</span>, <span class="hljs-number">4</span>, <span class="hljs-number">1</span>]  <span class="hljs-comment"># 输入层 2 个神经元，两个隐藏层各 4 个神经元，输出层 1 个神经元</span><br><br><span class="hljs-comment"># 训练网络</span><br>learning_rate = <span class="hljs-number">0.1</span><br>epochs = <span class="hljs-number">10000</span><br>parameters = train(X, Y, layer_dims, learning_rate, epochs)<br><br><span class="hljs-comment"># 预测</span><br>predictions = predict(X, parameters)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Predictions:&quot;</span>, predictions)<br></code></pre></td></tr></table></figure></div><hr><div><div class="post-metas my-3"><div class="post-meta mr-3 d-flex align-items-center"><i class="iconfont icon-category"></i> <span class="category-chains"><span class="category-chain"><a href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" class="category-chain-item">人工智能</a></span></span></div></div><div class="license-box my-3"><div class="license-title"><div>神经网络基础（带数学公式、代码）</div><div>https://jimes.cn/2025/01/19/神经网络基础（带数学公式）/</div></div><div class="license-meta"><div class="license-meta-item"><div>作者</div><div>Jimes</div></div><div class="license-meta-item license-meta-date"><div>发布于</div><div>2025年1月19日</div></div><div class="license-meta-item"><div>许可协议</div><div><a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by-nc-sa/4.0/"><span class="hint--top hint--rounded" aria-label="BY - 署名"><i class="iconfont icon-by"></i> </span></a><a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by-nc-sa/4.0/"><span class="hint--top hint--rounded" aria-label="NC - 非商业性使用"><i class="iconfont icon-nc"></i> </span></a><a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by-nc-sa/4.0/"><span class="hint--top hint--rounded" aria-label="SA - 相同方式共享"><i class="iconfont icon-sa"></i></span></a></div></div></div><div class="license-icon iconfont"></div></div><div class="post-prevnext my-3"><article class="post-prev col-6"><a href="/2025/01/21/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" title="卷积神经网络（CNN，包含代码实现）"><i class="iconfont icon-arrowleft"></i> <span class="hidden-mobile">卷积神经网络（CNN，包含代码实现）</span> <span class="visible-mobile">上一篇</span></a></article><article class="post-next col-6"><a href="/2025/01/11/%E6%95%B0%E6%8D%AE%E5%BA%93/" title="数据库系统概论笔记"><span class="hidden-mobile">数据库系统概论笔记</span> <span class="visible-mobile">下一篇</span> <i class="iconfont icon-arrowright"></i></a></article></div></div><article id="comments" lazyload><script type="text/javascript">Fluid.utils.loadComments("#comments",function(){var t="github-light",e="github-dark",s="dark"===(s=document.documentElement.getAttribute("data-user-color-scheme"))?e:t,t=(window.UtterancesThemeLight=t,window.UtterancesThemeDark=e,document.createElement("script"));t.setAttribute("src","https://utteranc.es/client.js"),t.setAttribute("repo","jiems3/utterances_comments"),t.setAttribute("issue-term","pathname"),t.setAttribute("label","utterances"),t.setAttribute("theme",s),t.setAttribute("crossorigin","anonymous"),document.getElementById("comments").appendChild(t)})</script><script src="https://utteranc.es/client.js" repo="jimes3/utterances_comments" issue-term="pathname" theme="github-light" crossorigin="anonymous" async></script><noscript>Please enable JavaScript to view the comments</noscript></article></article></div></div></div><div class="side-col d-none d-lg-block col-lg-2"><aside class="sidebar" style="margin-left:-1rem"><div id="toc"><p class="toc-header"><i class="iconfont icon-list"></i> <span>目录</span></p><div class="toc-body" id="toc-body"></div></div></aside></div></div></div><a id="scroll-top-button" aria-label="TOP" href="#" role="button"><i class="iconfont icon-arrowup" aria-hidden="true"></i></a><div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable modal-lg" role="document"><div class="modal-content"><div class="modal-header text-center"><h4 class="modal-title w-100 font-weight-bold">搜索</h4><button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button></div><div class="modal-body mx-3"><div class="md-form mb-5"><input type="text" id="local-search-input" class="form-control validate"> <label data-error="x" data-success="v" for="local-search-input">关键词</label></div><div class="list-group" id="local-search-result"></div></div></div></div></div></main><footer><div class="footer-inner"><div><span id="timeDate">正在载入天数...</span> <span id="times">载入时分秒...</span><script>var now=new Date;function createtime(){var n=new Date("01/01/2024 00:00:00");now.setTime(now.getTime()+250),days=(now-n)/1e3/60/60/24,dnum=Math.floor(days),hours=(now-n)/1e3/60/60-24*dnum,hnum=Math.floor(hours),1==String(hnum).length&&(hnum="0"+hnum),minutes=(now-n)/1e3/60-1440*dnum-60*hnum,mnum=Math.floor(minutes),1==String(mnum).length&&(mnum="0"+mnum),seconds=(now-n)/1e3-86400*dnum-3600*hnum-60*mnum,snum=Math.round(seconds),1==String(snum).length&&(snum="0"+snum),document.getElementById("timeDate").innerHTML="🚀已持续航行&nbsp"+dnum+"&nbsp天",document.getElementById("times").innerHTML=hnum+"&nbsp时&nbsp"+mnum+"&nbsp分&nbsp"+snum+"&nbsp秒"}setInterval("createtime()",250)</script></div><div class="total-wordcount">总字数: <span>57.9k</span> 字</div><div class="footer-content"><a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Jimes</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Blog</span></a></div><div class="statistics"><span id="busuanzi_container_site_pv" style="display:none">总访问量 <span id="busuanzi_value_site_pv"></span> 次 </span><span id="busuanzi_container_site_uv" style="display:none">总访客数 <span id="busuanzi_value_site_uv"></span> 人</span></div></div></footer><script src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js"></script><link rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css"><script>NProgress.configure({showSpinner:!1,trickleSpeed:100}),NProgress.start(),window.addEventListener("load",function(){NProgress.done()})</script><script src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js"></script><script src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js"></script><script src="/js/events.js"></script><script src="/js/plugins.js"></script><script src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js"></script><script>!function(t){var e=Fluid.plugins.typing,t=t.getElementById("subtitle");t&&e&&e(t.getAttribute("data-typed-text"))}((window,document))</script><script src="/js/img-lazyload.js"></script><script>Fluid.utils.createScript("https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js",function(){var t,o=jQuery("#toc");0!==o.length&&window.tocbot&&(t=jQuery("#board-ctn").offset().top,window.tocbot.init(Object.assign({tocSelector:"#toc-body",contentSelector:".markdown-body",linkClass:"tocbot-link",activeLinkClass:"tocbot-active-link",listClass:"tocbot-list",isCollapsedClass:"tocbot-is-collapsed",collapsibleClass:"tocbot-is-collapsible",scrollSmooth:!0,includeTitleTags:!0,headingsOffset:-t},CONFIG.toc)),0<o.find(".toc-list-item").length&&o.css("visibility","visible"),Fluid.events.registerRefreshCallback(function(){var t;"tocbot"in window&&(tocbot.refresh(),0!==(t=jQuery("#toc")).length)&&tocbot&&0<t.find(".toc-list-item").length&&t.css("visibility","visible")}))})</script><script src="https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js"></script><script>Fluid.plugins.codeWidget()</script><script>Fluid.utils.createScript("https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js",function(){window.anchors.options={placement:CONFIG.anchorjs.placement,visible:CONFIG.anchorjs.visible},CONFIG.anchorjs.icon&&(window.anchors.options.icon=CONFIG.anchorjs.icon);var n,o=[];for(n of(CONFIG.anchorjs.element||"h1,h2,h3,h4,h5,h6").split(","))o.push(".markdown-body > "+n.trim());"left"===CONFIG.anchorjs.placement&&(window.anchors.options.class="anchorjs-link-left"),window.anchors.add(o.join(", ")),Fluid.events.registerRefreshCallback(function(){if("anchors"in window){anchors.removeAll();var n,o=[];for(n of(CONFIG.anchorjs.element||"h1,h2,h3,h4,h5,h6").split(","))o.push(".markdown-body > "+n.trim());"left"===CONFIG.anchorjs.placement&&(anchors.options.class="anchorjs-link-left"),anchors.add(o.join(", "))}})})</script><script>Fluid.utils.createScript("https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js",function(){Fluid.plugins.fancyBox()})</script><script>Fluid.plugins.imageCaption()</script><script>window.MathJax?(MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typeset(),MathJax.typesetPromise()):window.MathJax={tex:{inlineMath:{"[+]":[["$","$"]]}},loader:{load:["ui/lazy"]},options:{renderActions:{insertedScript:[200,()=>{document.querySelectorAll("mjx-container").forEach(t=>{t=t.parentNode;"li"===t.nodeName.toLowerCase()&&t.parentNode.classList.add("has-jax")})},"",!1]}}},Fluid.events.registerRefreshCallback(function(){"MathJax"in window&&MathJax.startup.document&&"function"==typeof MathJax.startup.document.state&&(MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typeset(),MathJax.typesetPromise())})</script><script src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js"></script><script src="/js/local-search.js"></script><script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="/js/boot.js"></script><noscript><div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div></noscript><style>[bg-lazy]{background-image:none!important;background-color:#eee!important}</style><script>window.imageLazyLoadSetting={isSPA:!1,preloadRatio:1,processImages:null}</script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(e.href.match(t)||e.href.match(r))&&(e.href=a.dataset.original)})})</script><script>!function(r){r.imageLazyLoadSetting.processImages=t;var e=r.imageLazyLoadSetting.isSPA,n=r.imageLazyLoadSetting.preloadRatio||1,c=a();function a(){var t=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")),e=Array.prototype.slice.call(document.querySelectorAll("[bg-lazy]"));return t.concat(e)}function t(){e&&(c=a());for(var t,o=0;o<c.length;o++)0<=(t=(t=c[o]).getBoundingClientRect()).bottom&&0<=t.left&&t.top<=(r.innerHeight*n||document.documentElement.clientHeight*n)&&function(){var t,e,n,a=c[o],i=function(){c=c.filter(function(t){return a!==t}),r.imageLazyLoadSetting.onImageLoaded&&r.imageLazyLoadSetting.onImageLoaded(a)};(t=a).hasAttribute("bg-lazy")?(t.removeAttribute("bg-lazy"),i()):(e=new Image,n=t.getAttribute("data-original"),e.onload=function(){t.src=n,t.removeAttribute("data-original"),i()},t.src!==n&&(e.src=n))}()}function i(){clearTimeout(t.tId),t.tId=setTimeout(t,500)}t(),document.addEventListener("scroll",i),r.addEventListener("resize",i),r.addEventListener("orientationchange",i)}(this)</script></body></html>